{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 Индекс\n",
    "\n",
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### работа с файлами и папками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "filepath = os.path.join(curr_dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.path  \n",
    "путь до файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возвращает полный путь до папки/файла по имени файла / папки\n",
    "print(os.path.abspath(filepath))\n",
    "\n",
    "\n",
    "# возвращает имя файла / папки по полному ти до него\n",
    "print(os.path.basename(filepath))\n",
    "\n",
    "\n",
    "# проверить существование директории - True / False\n",
    "print(os.path.exists(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.listdir  \n",
    "возвращает список файлов в данной директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обходе файлов не забывайте исключать системные директории, такие как .DS_Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.walk\n",
    "root - начальная директория  \n",
    "dirs - список поддиректорий (папок)   \n",
    "files - список файлов в этих поддиректориях  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(curr_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __os.walk__ возвращает генератор, это значит, что получить его элементы можно только проитерировавшись по нему  \n",
    "но его легко можно превратить в list и увидеть все его значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(os.walk(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### чтение файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'test.txt'\n",
    "\n",
    "\n",
    "# одним массивом  \n",
    "with open(fpath, 'r') as f:  \n",
    "    text = f.read() \n",
    "\n",
    "    \n",
    "#по строкам, в конце каждой строки \\n  \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.readlines() \n",
    "\n",
    "    \n",
    "#по строкам, без \\n   \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про enumerate:    \n",
    "> При итерации по списку вы можете помимо самого элемента получить его порядковый номер    \n",
    "``` for i, element in enumerate(your_list): ...  ```    \n",
    "Иногда для получения элемента делают так -  ``` your_list[i] ```, не надо так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Индекс \n",
    "\n",
    "Сам по себе индекс - это просто формат хранения данных, он не может осуществлять поиск. Для этого необходимо добавить к нему определенную метрику. Это может быть что-то простое типа булева поиска, а может быть что-то более специфическое или кастомное под задачу.\n",
    "\n",
    "Давайте посмотрим, что полезного можно вытащить из самого индекса.    \n",
    "По сути, индекс - это информация о частоте встречаемости слова в каждом документе.   \n",
    "Из этого можно понять, например:\n",
    "1. какое слово является самым часто употребимым / редким\n",
    "2. какие слова встречаются всегда вместе - так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи\n",
    "3. как эти документы кластеризуются по N тематикам согласно словам, которые в них упоминаются "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Задача__: \n",
    "\n",
    "**Data:** Коллекция субтитров сезонов Друзьей. Одна серия - один документ.\n",
    "\n",
    "**To do:** Постройте небольшой модуль поискового движка, который сможет осуществлять поиск по коллекции документов.\n",
    "На входе запрос и проиндексированная коллекция (в том виде, как посчитаете нужным), на выходе отсортированный по релевантности с запросом список документов коллекции. \n",
    "\n",
    "Релизуйте:\n",
    "    - функцию препроцессинга данных\n",
    "    - функцию индексирования данных\n",
    "    - функцию метрики релевантности \n",
    "    - собственно, функцию поиска\n",
    "\n",
    "[download_friends_corpus](https://yadi.sk/d/yVO1QV98CDibpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про defaultdict: \n",
    "> В качестве multiple values словаря рекомендую использовать ``` collections.defaultdict ```                          \n",
    "> Так можно избежать конструкции ``` dict.setdefault(key, default=None) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### _check : в коллекции должно быть около 165 файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью обратного индекса посчитайте:  \n",
    "\n",
    "\n",
    "a) какое слово является самым частотным\n",
    "\n",
    "b) какое самым редким\n",
    "\n",
    "c) какой набор слов есть во всех документах коллекции\n",
    "\n",
    "d) какой сезон был самым популярным у Чендлера? у Моники?\n",
    "\n",
    "e) кто из главных героев статистически самый популярный? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \"\\\n",
    "              and token.strip() not in punctuation] \n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "fil = []\n",
    "for root, dirs, files in os.walk('friends'):\n",
    "    for file in files:\n",
    "        if file[0] != '.':\n",
    "            fil.append(os.path.join(root, file))\n",
    "            file = open(os.path.join(root, file), 'r', encoding='utf-8')\n",
    "            text = file.read()\n",
    "            text = process(text)\n",
    "            corpus.append(text)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeciese(corpus):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    Y = vectorizer.get_feature_names()\n",
    "    arr = X.toarray()\n",
    "    df = pd.DataFrame(data=arr, index=fil, columns=Y)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03472717, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03369391, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05011025, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.0352362 , 0.        , 0.        , ..., 0.        , 0.08312966,\n",
       "        0.        ],\n",
       "       [0.03397161, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03495448, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def relevance(data):\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    i = tfidf_transformer.fit(X)\n",
    "    m = tfidf_transformer.fit_transform(X).toarray()\n",
    "    return (i, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(quiry):\n",
    "    data = indeciese(corpus)\n",
    "    tfidf_transformer, tfidf_collection = relevance (data)\n",
    "    quiry = process(quiry)\n",
    "    quiry_ind = vectorizer.fit_transform(quiry)\n",
    "    quiry_ind = tfidf_transformer.fit_transform(quiry_ind).toarray()\n",
    "    tosort = []\n",
    "    for i in range(len(fil)):\n",
    "        q = [fil[i], scipy.spatial.distance.cosine[quiry_ind, tfidf_collection[0][i]]]\n",
    "        tosort.append(q)\n",
    "    return tosort.sort(key = sortSecond)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частотное слово это: это\n",
      "Слова, которые встречаются во всех текстах: каждый, какой\n"
     ]
    }
   ],
   "source": [
    "maximum = 0\n",
    "word = 0\n",
    "alltexts = []\n",
    "for col in df:\n",
    "    if df[col].sum() > maximum:\n",
    "        maximum = df[col].sum()\n",
    "        word = col\n",
    "    if df[col].sum() == 165:\n",
    "        alltexts.append(col)\n",
    "print ('Самое частотное слово это: ' + word)\n",
    "print ('Слова, которые встречаются во всех текстах: ' + ', '.join(alltexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово, которое встречается реже всего: 03815\n"
     ]
    }
   ],
   "source": [
    "minimum = 500\n",
    "word = 0\n",
    "for col in df:\n",
    "    if df[col].sum() < minimum:\n",
    "        minimum = df[col].sum()\n",
    "        word = col\n",
    "print ('Слово, которое встречается реже всего: ' + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['чендлер', 'моника']\n"
     ]
    }
   ],
   "source": [
    "names = ['Чендлер', 'Моника']\n",
    "nnames = []\n",
    "for i in names:\n",
    "    nnames.append(process(i))\n",
    "print (nnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = df['моника'][0:25].sum()\n",
    "s5 = df['моника'][25:49].sum()\n",
    "s4 = df['моника'][49:75].sum()\n",
    "s3 = df['моника'][75:96].sum()\n",
    "s6 = df['моника'][96:121].sum()\n",
    "s1 = df['моника'][121:141].sum()\n",
    "s7 = df['моника'][141:].sum()\n",
    "\n",
    "s2 = df['чендлер'][0:25].sum()\n",
    "s5 = df['чендлер'][25:49].sum()\n",
    "s4 = df['чендлер'][49:75].sum()\n",
    "s3 = df['чендлер'][75:96].sum()\n",
    "s6 = df['чендлер'][96:121].sum()\n",
    "s1 = df['чендлер'][121:141].sum()\n",
    "s7 = df['чендлер'][141:].sum()\n",
    "\n",
    "#Ответ: Моника - 7\n",
    "#Ответ: Чендлер - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Рейчел', 'Моника', 'Фиби', 'Джоуи', 'Чендлер', 'Росс']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main = 'Рейчел, Моника, Фиби, Джоуи, Чендлер, Росс'\n",
    "main = main.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "росс самый популярый персонаж\n"
     ]
    }
   ],
   "source": [
    "ma = 0\n",
    "name = 0\n",
    "for nam in main:\n",
    "    if df[nam].sum() > ma:\n",
    "        ma = df[nam].sum()\n",
    "        name = nam\n",
    "print (name + ' самый популярый персонаж')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
